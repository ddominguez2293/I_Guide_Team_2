{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a6bc32",
   "metadata": {},
   "source": [
    "# Data Processing for Tabular Modeling\n",
    "This notebook provides a **step-by-step walkthrough** of the data preprocessing pipeline. The goal is to prepare the dataset for machine learning by cleaning, feature engineering, and saving the processed data.\n",
    "\n",
    "### Overview of Steps:\n",
    "1. Import required libraries\n",
    "2. Load raw data\n",
    "3. Perform exploratory checks (shape, preview, missing values)\n",
    "4. Handle missing values and clean data\n",
    "5. Feature engineering\n",
    "6. Save processed dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6d047",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b6676",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ee, geemap\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "from os.path import join\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# ee.Authenticate(force=True)\n",
    "# ee.Initialize(project=\"your-project-id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dedd5f5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Download Satellite Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2891792",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL');\n",
    "aoi = ee.Geometry.BBox(north=-13.450000, south=-23.929167, west=-61.20833, east=-52.204167);\n",
    "image = dataset.filterDate('2024-01-01', '2025-01-01').filterBounds(aoi).mosaic()\n",
    "\n",
    "geemap.ee_export_image_to_drive(\n",
    "    image=image,\n",
    "    description=f'Embedding',\n",
    "    folder='IGUIDE',\n",
    "    region=aoi,\n",
    "    dimensions=\"2161x2515\",\n",
    "    maxPixels=1e10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95648ab3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T01:08:38.308764Z",
     "start_time": "2025-08-08T01:08:38.304568Z"
    }
   },
   "source": [
    "# Estimate Burn History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9975a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_area = xr.open_dataset(\"/anvil/projects/x-cis250634/team2/Final_Data/Burn area/MCD64A1.061_500m_aid0001_202405_202407.nc\", engine='rasterio',decode_times=True)\n",
    "# hold time-indexed burn layers\n",
    "burn_qc_list = []\n",
    "\n",
    "for t in range(burn_area.sizes[\"time\"]):\n",
    "    burn_doy = burn_area[\"Burn_Date\"][t, :, :]\n",
    "    uncertainty = burn_area[\"Burn_Date_Uncertainty\"][t, :, :]\n",
    "    qa = burn_area[\"QA\"][t, :, :]\n",
    "\n",
    "    # Decode QA\n",
    "    burned = burn_doy > 0\n",
    "    good_uncertainty = uncertainty <= 5.0\n",
    "    valid_data = (qa.astype(int) & (1 << 1)) > 0\n",
    "    special_condition = (((qa.astype(int).values >> 5) & 0b111) == 0)\n",
    "    qc_mask = burned & good_uncertainty & valid_data & special_condition\n",
    "\n",
    "    # Mask the DOY values\n",
    "    burn_doy_qc_masked = burn_doy.where(qc_mask, np.nan)\n",
    "\n",
    "    # Create a DataArray with lat/lon dimensions\n",
    "    burn_doy_qc = xr.DataArray(\n",
    "        burn_doy_qc_masked.values,\n",
    "        coords={'y': burn_doy_qc_masked.y, 'x': burn_doy_qc_masked.x},\n",
    "        dims=['y', 'x']\n",
    "    )\n",
    "\n",
    "    # Add time coordinate for stacking\n",
    "    burn_doy_qc = burn_doy_qc.expand_dims(dim={\"time\": [burn_area[\"time\"][t].values]})\n",
    "\n",
    "    burn_qc_list.append(burn_doy_qc)\n",
    "\n",
    "# Stack along time dimension\n",
    "burn_qc_all = xr.concat(burn_qc_list, dim=\"time\")\n",
    "burn_qc_all.name = \"Burn_DOY_QC\"\n",
    "\n",
    "# Write CRS to the final stacked DataArray\n",
    "burn_qc_all.rio.write_crs(\"EPSG:4326\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111aee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doys = range(121, 213)\n",
    "lags = range(0, 6)\n",
    "patch = 5\n",
    "\n",
    "burn_roll_list = []\n",
    "for i in range(burn_area.sizes[\"time\"]):\n",
    "    burn_doy = burn_qc_all.sel(time=i)\n",
    "    doys = np.unique(burn_doy.values)\n",
    "    for doy in tqdm(doys):\n",
    "        for t in lags:\n",
    "            burn_roll = (burn_doy==(doy-t)).astype(int).rolling(dim={\"x\":patch,\"y\":patch}, center=True).sum()\n",
    "            burn_roll = burn_roll.expand_dims({\"doy\": 1, \"lag\": 1}).assign_coords({\"doy\": [doy], \"lag\": [t]})\n",
    "            burn_roll_list.append(burn_roll)\n",
    "\n",
    "burn_roll_all = xr.combine_by_coords(burn_roll_list)\n",
    "\n",
    "joblib.dump(burn_roll_all, f\"/anvil/projects/x-cis250634/team2/Final_Data/Rolling_Data/Burn_Roll.lzma\")\n",
    "burn_roll = joblib.load(f\"/anvil/projects/x-cis250634/team2/Final_Data/Rolling_Data/Burn_Roll.lzma\")\n",
    "burn_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ee1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBurnRoll(doy):\n",
    "    da_doy = burn_roll.sel(doy=doy)\n",
    "    da_doy = da_doy.transpose('y', 'x', 'lag')\n",
    "    df = da_doy.to_dataframe().reset_index()\n",
    "    df_pivot = df.pivot_table(index=[\"x\", \"y\"], columns='lag', values='Burn_DOY_QC', dropna=False, fill_value=100)\n",
    "    df_pivot.columns =[f'lag_{int(c)}' for c in df_pivot.columns]\n",
    "    df_pivot.index.names = [\"lon\", \"lat\"]\n",
    "    df_sorted = df_pivot.sort_index().astype(\"uint8\")\n",
    "    df_sorted.to_parquet(join(out_path, f\"burn_roll_{doy}.parquet\"), compression=\"gzip\")\n",
    "\n",
    "for doy in tqdm(burn_roll[\"doy\"].values): processBurnRoll(doy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9fce9b",
   "metadata": {},
   "source": [
    "# Process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = rxr.open_rasterio(r\"/anvil/projects/x-cis250634/team2/Final_Data/Embedding/Embedding.tif\")\n",
    "\n",
    "mean_da = embedding.mean(dim='band', keep_attrs=True).expand_dims(band=[65])\n",
    "min_da = embedding.min(dim='band', keep_attrs=True).expand_dims(band=[66])\n",
    "max_da = embedding.max(dim='band', keep_attrs=True).expand_dims(band=[67])\n",
    "std_da = embedding.std(dim='band', keep_attrs=True).expand_dims(band=[68])\n",
    "embedding_extended = xr.concat([embedding, mean_da, min_da, max_da, std_da], dim='band')\n",
    "\n",
    "ds = embedding_extended.to_dataset(name='band_values')\n",
    "ds = ds.transpose('y', 'x', 'band')\n",
    "df = ds.to_dataframe().reset_index()\n",
    "df = df.pivot(index=['y', 'x'], columns='band', values='band_values').reset_index()\n",
    "df.columns.name = None\n",
    "df = df.rename(columns={'x': 'lon', 'y': 'lat'})\n",
    "df.columns = ['lat', 'lon'] + [f'band_{i}' for i in range(68)]\n",
    "\n",
    "df = df.rename(columns={\"band_64\": \"embed_mean\", \"band_65\": \"embed_min\", \"band_66\": \"embed_max\", \"band_67\": \"embed_std\"})\n",
    "df = df.set_index([\"lon\", \"lat\"]).sort_index().astype(\"float32\")\n",
    "df.to_parquet(f\"/anvil/projects/x-cis250634/team2/Final_Data/Embedding/embedding.parquet\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"/anvil/projects/x-cis250634/team2/Final_Data/CSV/*.csv\"\n",
    "csv_files = glob(file_path)\n",
    "cols = ['grid_id', 'lon', 'lat', 'land_cover', 'dem', 't2m', 'd2m', 'u10', 'v10', 'tp', 'swvl1', 'sp', 'burn']\n",
    "\n",
    "idx_cols = ['lon', 'lat']\n",
    "int32_cols = [\"grid_id\"]\n",
    "int8_cols = [\"land_cover\", \"burn\"]\n",
    "float32_cols = ['dem', 't2m', 'd2m', 'u10', 'v10', 'tp', 'swvl1', 'sp']\n",
    "\n",
    "out_path = r\"D:\\Projects\\IGUIDE\\Datasets\\Raw_Data\"\n",
    "\n",
    "doys = pd.Series(csv_files)\n",
    "doys = doys.apply(lambda p: datetime.strptime(os.path.splitext(os.path.basename(p))[0], \"%Y-%m-%d\").timetuple().tm_yday).values\n",
    "\n",
    "def processRawData(path, doy):\n",
    "    df = pd.read_csv(path, usecols=cols)\n",
    "    df = pd.concat(    \n",
    "        [   df[idx_cols],\n",
    "            df[int32_cols].astype(\"uint32\", errors=\"ignore\"),\n",
    "            df[int8_cols].astype(\"uint8\", errors=\"ignore\"),\n",
    "            df[float32_cols].astype(\"float32\", errors=\"ignore\")],\n",
    "        axis=1)\n",
    "\n",
    "    df = df.set_index([\"lon\", \"lat\"]).sort_index()\n",
    "    df.to_parquet(join(out_path, f\"org_data_{doy-1}.parquet\"), compression=\"gzip\")\n",
    "\n",
    "for path, doy in tqdm(zip(csv_files[33:], doys[33:])): processRawData(path, doy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8fe3c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc258b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "main_dir = r\"/anvil/projects/x-cis250634/team2/Final_Data\"\n",
    "raw_dir = join(main_dir, \"Raw_Data\")\n",
    "rol_dir = join(main_dir, \"Rolling_Data\")\n",
    "emb_dir = join(main_dir, \"Embedding\")\n",
    "daily_dir = join(main_dir, \"Data\", \"Daily\")\n",
    "train_dir = join(main_dir, \"Data\", \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dfa90",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_burn = gpd.read_file(join(main_dir, \"MaxBurnedArea.zip\"))\n",
    "raw_data = pd.read_parquet(join(raw_dir, \"org_data_121.parquet\"))\n",
    "\n",
    "poi_data = raw_data.reset_index()[[\"lat\", \"lon\", \"grid_id\"]]\n",
    "poi_data = poi_data.sort_values([\"lat\", \"lon\",], ascending=[False, True])\n",
    "poi_data = gpd.GeoDataFrame(\n",
    "    data=poi_data,\n",
    "    geometry=gpd.points_from_xy(poi_data['lon'], poi_data['lat']),\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "\n",
    "filtered_points = poi_data[poi_data.within(max_burn.union_all())]\n",
    "filtered_points.to_parquet(join(main_dir, \"Points.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c7f07",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "embedding_data = (\n",
    "    pd.read_parquet(join(emb_dir, \"embedding.parquet\"))\n",
    "    .reset_index()\n",
    "    .sort_values([\"lat\", \"lon\"], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    "    .rename_axis('grid_id')\n",
    "    .drop(columns=[\"lat\", \"lon\"])\n",
    ")\n",
    "\n",
    "clipped_points = pd.read_parquet(join(main_dir, \"Points.parquet\"))[\"grid_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a966f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def combineData(doy):\n",
    "    raw_data = (\n",
    "        pd.read_parquet(join(raw_dir, f\"org_data_{doy}.parquet\"))\n",
    "        .reset_index()\n",
    "        .sort_values([\"lat\", \"lon\"], ascending=[False, True])\n",
    "        .assign(doy=doy)\n",
    "        .set_index([\"doy\", \"grid_id\"])\n",
    "        \n",
    "    )\n",
    "    \n",
    "    roll_data = (\n",
    "        pd.read_parquet(join(rol_dir, f\"burn_roll_{doy}.parquet\"))\n",
    "        .reset_index()\n",
    "        .sort_values([\"lat\", \"lon\"], ascending=[False, True])\n",
    "        .reset_index(drop=True)\n",
    "        .rename_axis('grid_id')\n",
    "        .assign(doy=doy)\n",
    "        .set_index(\"doy\", append=True)\n",
    "        .reorder_levels([\"doy\", \"grid_id\"])\n",
    "        .drop(columns=[\"lat\", \"lon\"])\n",
    "        \n",
    "    )\n",
    "    \n",
    "    emb_data = (\n",
    "        embedding_data\n",
    "        .assign(doy=doy)\n",
    "        .set_index(\"doy\", append=True)\n",
    "        .reorder_levels([\"doy\", \"grid_id\"])\n",
    "    )\n",
    "    \n",
    "    \n",
    "    data = pd.concat([raw_data, roll_data, emb_data], axis=1)\n",
    "    data.to_parquet(join(daily_dir, f\"data_{doy}.parquet\"), compression=\"gzip\")\n",
    "    \n",
    "\n",
    "def clipData(doy):\n",
    "    df = (\n",
    "        pd.read_parquet(join(daily_dir, f\"data_{doy}.parquet\"))\n",
    "        .replace(dict(zip(lags, [100]*len(lags))), np.nan)\n",
    "        .dropna(how=\"any\")\n",
    "    )\n",
    "    \n",
    "    df_whole = df.copy()\n",
    "    df.index.get_level_values(\"grid_id\")\n",
    "    df_clipped = df[df.index.get_level_values(\"grid_id\").isin(clipped_points)]\n",
    "    return df_whole, df_clipped\n",
    "\n",
    "def splitSample(df):\n",
    "    burn_subset = df[df[y_column] == 1]\n",
    "    nonburn_subset = df[df[y_column] == 0]\n",
    "\n",
    "    if not (burn_subset.empty or nonburn_subset.empty):   \n",
    "        n_burn = min(len(burn_subset), max_burn_per_day)\n",
    "        sampled_burns = burn_subset.sample(n=n_burn, random_state=42)\n",
    "        n_nonburn = min(len(nonburn_subset), nonburn_ratio * n_burn)\n",
    "        sampled_nonburns = nonburn_subset.sample(n=n_nonburn, random_state=42)\n",
    "        sample_df = pd.concat([sampled_burns, sampled_nonburns], axis=0)\n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84342774",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_column = 'burn'\n",
    "max_burn_per_day = 5000\n",
    "nonburn_ratio = 1\n",
    "lags = ['lag_0', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d680be5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "samples_clip = []\n",
    "samples_whole = []\n",
    "\n",
    "for doy in  tqdm(range(121, 213)):\n",
    "    df_whole, df_clip = clipData(doy)\n",
    "    sample_whole = splitSample(df_whole)\n",
    "    sample_clip = splitSample(df_clip)\n",
    "    samples_whole.append(sample_whole)\n",
    "    samples_clip.append(sample_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c79710",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_whole = pd.concat(samples_whole, axis=0).drop(columns={\"lag_0\"})\n",
    "training_clip = pd.concat(samples_clip, axis=0).drop(columns={\"lag_0\"})\n",
    "\n",
    "training_whole.to_parquet(join(train_dir, \"training_whole.parquet\"), compression=\"gzip\")\n",
    "training_clip.to_parquet(join(train_dir, \"training_clip.parquet\"), compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
